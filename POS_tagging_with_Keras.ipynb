{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "CUSTOM_SEED = 113\n",
    "np.random.seed(CUSTOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /Users/ragnarjok/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "sentences = treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3914"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_tags: 46ntags: {'-NONE-', ':', '$', 'LS', 'JJ', 'TO', 'POS', 'VBD', 'RP', 'PRP$', \"''\", 'MD', 'WDT', '-LRB-', 'NN', 'PDT', 'NNS', 'JJS', 'EX', 'VBN', 'JJR', 'SYM', 'IN', 'VBZ', ',', 'VBG', 'CD', '.', 'RB', 'WP', 'CC', 'PRP', 'NNPS', '-RRB-', 'WRB', 'WP$', 'RBS', '``', 'UH', '#', 'VBP', 'NNP', 'FW', 'VB', 'DT', 'RBR'}\n"
     ]
    }
   ],
   "source": [
    "tags = set([\n",
    "    tag for sentence in sentences \n",
    "    for _, tag in sentence\n",
    "])\n",
    "print('nb_tags: %sntags: %s' % (len(tags), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_basic_features(sentence_terms, sentence_length, index):\n",
    "    \"\"\" Compute some very basic word features.\n",
    "\n",
    "        :param sentence_terms: [w1, w2, ...] \n",
    "        :type sentence_terms: list\n",
    "        :param index: the index of the word \n",
    "        :type index: int\n",
    "        :return: dict containing features\n",
    "        :rtype: dict\n",
    "    \"\"\"\n",
    "    term = sentence_terms[index]\n",
    "    return {\n",
    "        'nb_terms': sentence_length,\n",
    "        'term': term,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == sentence_length - 1,\n",
    "        'is_capitalized': term[0].upper() == term[0],\n",
    "        'is_all_caps': term.upper() == term,\n",
    "        'is_all_lower': term.lower() == term,\n",
    "        'prefix-1': term[0],\n",
    "        'prefix-2': term[:2],\n",
    "        'prefix-3': term[:3],\n",
    "        'suffix-1': term[-1],\n",
    "        'suffix-2': term[-2:],\n",
    "        'suffix-3': term[-3:],\n",
    "        'prev_word': '' if index == 0 else sentence_terms[index - 1],\n",
    "        'next_word': '' if index == sentence_length - 1 else sentence_terms[index + 1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "    \"\"\" \n",
    "    Remove the tag for each tagged term.\n",
    "    :param tagged_sentence: a POS tagged sentence\n",
    "    :type tagged_sentence: list\n",
    "    :return: a list of tags\n",
    "    :rtype: list of strings\n",
    "    \"\"\"\n",
    "    return [w for w, _ in tagged_sentence]\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "    \"\"\"\n",
    "    Split tagged sentences to X and y datasets and append some basic features.\n",
    "    :param tagged_sentences: a list of POS tagged sentences\n",
    "    :param tagged_sentences: list of list of tuples (term_i, tag_i)\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for pos_tags in tagged_sentences:\n",
    "        untagged = untag(pos_tags)\n",
    "        untagged_length = len(untagged)\n",
    "        for index, (term, class_) in enumerate(pos_tags):\n",
    "            # Add basic NLP features for each sentence term\n",
    "            X.append(add_basic_features(untagged, untagged_length, index))\n",
    "            y.append(class_)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_X, ds_y = transform_to_dataset(sentences)\n",
    "tokens_count = len(ds_X)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# Fit our DictVectorizer with our set of features\n",
    "dict_vectorizer = DictVectorizer(sparse=False)\n",
    "dict_vectorizer.fit(ds_X)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Fit LabelEncoder with our list of classes\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(ds_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "def normalize_X_y(ds_X, ds_y, start, end):\n",
    "    X = dict_vectorizer.transform(ds_X[start:end])\n",
    "    y = np_utils.to_categorical(label_encoder.transform(ds_y[start:end]))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "start_1 = int(tokens_count * 0.5)\n",
    "start_2 = int(tokens_count * 0.75)\n",
    "end = tokens_count \n",
    "\n",
    "X_train, y_train = normalize_X_y(ds_X, ds_y, start, start_1)\n",
    "X_test, y_test =normalize_X_y(ds_X, ds_y, start_1, start_2)\n",
    "X_val, y_val = normalize_X_y(ds_X, ds_y, start_2, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "def build_model(input_dim, hidden_neurons, output_dim):\n",
    "    \"\"\"\n",
    "    Construct, compile and return a Keras model which will be used to fit/predict\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(hidden_neurons),\n",
    "        Activation('relu'),\n",
    "        Dropout(0.75),\n",
    "        Dense(output_dim, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "model_params = {\n",
    "    'build_fn': build_model,\n",
    "    'input_dim': X_train.shape[1],\n",
    "    'hidden_neurons': 512,\n",
    "    'output_dim': y_train.shape[1],\n",
    "    'epochs': 4,\n",
    "    'batch_size': 128,\n",
    "    'verbose': 1,\n",
    "    'validation_data': (X_val, y_val),\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "clf = KerasClassifier(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50338 samples, validate on 25169 samples\n",
      "Epoch 1/4\n",
      "50338/50338 [==============================] - 225s 4ms/step - loss: 1.0311 - acc: 0.7353 - val_loss: 0.2526 - val_acc: 0.9211\n",
      "Epoch 2/4\n",
      "50338/50338 [==============================] - 220s 4ms/step - loss: 0.2620 - acc: 0.9259 - val_loss: 0.1779 - val_acc: 0.9415\n",
      "Epoch 3/4\n",
      "17152/50338 [=========>....................] - ETA: 2:02 - loss: 0.1631 - acc: 0.9523"
     ]
    }
   ],
   "source": [
    "hist = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_model_performance(train_loss, train_acc, train_val_loss, train_val_acc):\n",
    "    \"\"\" Plot model loss and accuracy through epochs. \"\"\"\n",
    "\n",
    "    blue= '#34495E'\n",
    "    green = '#2ECC71'\n",
    "    orange = '#E23B13'\n",
    "\n",
    "    # plot model loss\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(10, 8))\n",
    "    ax1.plot(range(1, len(train_loss) + 1), train_loss, blue, linewidth=5, label='training')\n",
    "    ax1.plot(range(1, len(train_val_loss) + 1), train_val_loss, green, linewidth=5, label='validation')\n",
    "    ax1.set_xlabel('# epoch')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.tick_params('y')\n",
    "    ax1.legend(loc='upper right', shadow=False)\n",
    "    ax1.set_title('Model loss through #epochs', color=orange, fontweight='bold')\n",
    "\n",
    "    # plot model accuracy\n",
    "    ax2.plot(range(1, len(train_acc) + 1), train_acc, blue, linewidth=5, label='training')\n",
    "    ax2.plot(range(1, len(train_val_acc) + 1), train_val_acc, green, linewidth=5, label='validation')\n",
    "    ax2.set_xlabel('# epoch')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "    ax2.tick_params('y')\n",
    "    ax2.legend(loc='lower right', shadow=False)\n",
    "    ax2.set_title('Model accuracy through #epochs', color=orange, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "plot_model_performance(\n",
    "    train_loss=hist.history.get('loss', []),\n",
    "    train_acc=hist.history.get('acc', []),\n",
    "    train_val_loss=hist.history.get('val_loss', []),\n",
    "    train_val_acc=hist.history.get('val_acc', [])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.model.save('/tmp/keras_mlp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
